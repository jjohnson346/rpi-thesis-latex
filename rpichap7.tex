%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%                                                                 %
%                            CHAPTER SEVEN  - Conclusion and Future Work            %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
 

\chapter{Conclusion and Future Work}

This chapter summarizes the work we’ve covered in this dissertation and offers areas for further investigation.

\section{Summary of Work}

The research in this dissertation began with developing a battery of multiple choice questions from the CFE exam materials and the preparation of the CFE Manual text for test processing by an automated agent. Next, we presented Version 1 of the CFE agent which used very primitive rule-based algorithms to answer questions but which were nonetheless quite effective relative to random guessing, and the results proved statistically significant at the 99\% level. Unfortunately, justifications for the agent’s answers left much to be desired - offering no more justification than that for choosing a particular algorithm based on optimal performance in the training set.  

In Version 2 of the agent, more sophisticated algorithms were brought to bear on the problem using information retrieval for which we saw marked improvement in accuracy (a jump from roughly 49\% accuracy to 59.5\%, and in particular, a 70\% accuracy rate for ConceptMatchV3 on definition questions) and a more (but not completely) transparent justification for the agent’s answers. Here, though, the justifications in the agent’s answers were still not completely transparent as the coefficients upon which the cosine similarity computations were based were derived from digesting term counts within the documents of the document collection. And so, justifications were limited to demonstrating how coefficients determined at the aggregate level were applied to arrive at the answer for each individual question.

Version 3 of the agent used machine learning to isolate the passages (paragraphs) from among the documents in the document collection relevant to each definition question. This technique provided 93\% probability that the correct passage was within the selection of top seven passages as ranked by the logistic regression
algorithm in the test set. Although the relatively simplistic answer extraction algorithms that utilized the results of the passage selection algorithm yielded roughly 69\% accuracy on the definition questions in the test set, we believe further work to refine these answer extraction algorithms will result in significant accuracy gains, surpassing those of Version 2 at a statistically significant level. However, as we saw with Version 2, this approach offered only limited transparency into the reasoning by the agent, limited to the application of opaquely determined coefficients at the aggregate level to each individual problem.

Finally, in Version 4 of the agent, we looked at utilizing semantic reasoning in the agent. This approach offered the benefit of completely transparent reasoning based on natural deduction using the $\mathcal{DCEC}^\ast$.  However, our work here highlighted an important issue with this approach -- the problem of translating assertions expressed in natural language into a formal language, (in our case, again, the language of choice was the $\mathcal{DCEC}^\ast$).  This problem exists for both translating assertions in the information source (the CFE Manual, in our example), and for translating declaration germane to the problem at hand. In the example we covered in Chapter 6, these were the assertions associated with Blue and his interactions with Dr. White and Dr. Black.

\section{Future Work}

Certainly, as alluded to in the above paragraphs, the incorporation of machine learning into passage selection offered considerable promise for refining the algorithms of the CFE agent. So, continuing along this path, further refinement of the answer extraction algorithms would also likely deliver improved overall accuracy, as discussed above. Specifically, perhaps incorporation of machine learning in the answer extraction algorithm as well as the passage selection algorithm may prove beneficial. This is a tact that will be pursued in future work.

Now, consider the following problem from the CFE test set:

\blockquote{True or False: Green, an agent for the White Corporation, acting within the scope of his duties and for the benefit of White, committed an act of fraud. According to the Sentencing Guidelines effective in 1991, White cannot be held criminally liable for Green’s actions.}

This is a true/false question, a type that was not specifically targeted in any of the first three versions of the CFE agent. The reason for that is obvious – this is a tough problem for the CFE Agent using the algorithms of Versions 1, 2, and 3 where the algorithms are looking for the juxtaposition of key words of the question stem and those of the various answer options within the CFE Manual. In a true/false problem, however, answer options are of no help, leaving the agent with only the question stem from which it must discern whether the statement is valid.  Proximity of key words will likely not work here as it is likely that such questions rich with keywords from a given section are as likely to be invalid as valid. (This statement is merely conjecture at this point. Further work needs to be done here to verify this assertion scientifically.)

An algorithm that is based on a semantic understanding of the problem would certainly appear useful, here. But for that, we'd need the tools for translating natural language assertions to ones expressed in a formal language, where in our case, we'd recommend the $\mathcal{DCEC}^\ast$ due to its built-in support for modeling the cognitive states and interactions typical of agents in financial fraud scenarios. There are two possible approaches for tackling this task. The first approach to the translation problem is a computational semantics approach based on Blackburn and Bos \cite{blackburn_2005_representation_ch2}, wherein the authors explain an approach to the representation of natural language assertions in first-order logic ($\mathcal{FOL}$) using the lambda calculus. In the approach we propose here, however, the target representation language would be the $\mathcal{DCEC}^\ast$, not $\mathcal{FOL}$.  A second approach would involve parsing the sentences first, using a probabilistic parser \cite{martin_2000_speech_ch14}, such as OpenNLP (https://opennlp.apache.org) or the Stanford Parser (http://nlp.stanford.edu/software/lex-parser.shtml),  and then translating the parse structures into assertions expressedin the $\mathcal{DCEC}^\ast$.  The idea here is that both the natural language utterances and the parse structures serve as inputs to the computational semantics \cite{blackburn_2005_representation_ch2} model that produces the output assertions in the $\mathcal{DCEC}^\ast$.  Having parse structures as inputs may be helpful in better capturing the semantics of the utterance in the semantics model. 

As a final thought, we believe that any comprehensive QA system that provides both answers with a high degree of accuracy along with justifications, a holistic approach would be required – one that leverages extended versions of the machine learning/information retrieval approaches in Versions 2 and 3 of the agent as well as the semantic approach of Version 4. How might that be accomplished? Depending on the type of question, this holistic approach could take on different forms. For true/false questions, as we have seen, machine learning approaches may be of limited use, and thus an approach more centered around semantic computation is required.  However, for definition questions, a high level approach might be to leverage the machine learning approach to arrive at an answer, and then use that answer to engineer the justification using a semantic approach.  The idea here is to utilize the machine learning approaches for finding the answers and the semantic computational approach for explaining them. For other types of questions, still yet a different blend of these approaches may be required.