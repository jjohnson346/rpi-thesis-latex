%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%                                                                 %
%                            CHAPTER SEVEN  - Conclusion and Future Work            %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
 

\chapter{Conclusion and Future Work}

This chapter summarizes the work we’ve covered in this dissertation and offers areas for further investigation.

\section{Summary of Work}

The research underpinning this dissertation began with developing a battery of multiple choice questions from the CFE exam materials and the preparation of the CFE Manual text for test processing by an automated agent. This laid the groundwork for the development of the agents discussed in detail above for which a comparative analysis of performance and reasoning transparency could be accomplished.  Version 1 of the CFE agent uses only primitive rule-based algorithms, but performs effectively, showing improved performance over random guessing at the 99\% level of statistical significance.  Unfortunately, justifications for the agent’s answers are minimal, offering little more explanation than the choice of a particular algorithm based on relative performance in the training set.  

In Version 2 of the agent, more sophisticated algorithms are brought to bear on the problem using information retrieval for which we see marked improvement in accuracy (a jump from roughly 49\% accuracy to 59.5\%, and in particular, a 70\% accuracy rate for ConceptMatchV3 on definition questions) and a more transparent justification for the agent’s answers. However, the justifications in the agent's answers are still not \emph{completely} transparent as the coefficients upon which the cosine similarity computations are based are derived from term counts within the documents of the document collection. And so, justifications are thus limited to showing how these coefficients determined at the aggregate level are applied at the level of each individual question.

Version 3 of the agent uses machine learning to isolate the passages (paragraphs) from among the documents in the document collection relevant to each definition question. This technique provides 93\% probability that the correct passage is within the selection of top seven passages as ranked by the logistic regression
algorithm in the test set. Although the relatively simplistic answer extraction algorithms that utilize the results of the passage selection algorithm yield roughly 69\% accuracy on the definition questions in the test set, we believe further work to refine these answer extraction algorithms will result in significant accuracy gains, surpassing those of Version 2 at a statistically significant level. However, as we have observed with Version 2, this approach offers only limited transparency into the reasoning by the agent, limited to the application of opaquely determined coefficients at the aggregate level to each individual problem.

Finally, in Version 4 of the agent, we look at utilizing semantic reasoning. This approach offers the benefit of completely transparent reasoning based on natural deduction using the $\mathcal{DCEC}^\ast$.  However, our work, here, highlights an important issue with this approach -- the problem of translating assertions expressed in natural language into a formal language, (in our case, again, the language of choice was the $\mathcal{DCEC}^\ast$).  This problem exists for both translating assertions in the information source (the CFE Manual, in our example), and for translating declarations germane to the problem at hand. In the example we covered in Chapter 6, these were the assertions associated with Blue and his interactions with Dr. White and Dr. Black.

\section{Future Work}

Certainly, as alluded to in the above paragraphs, the incorporation of machine learning into passage selection offers considerable promise for refining the algorithms of the CFE agent. So, continuing along this path, further refinement of the answer extraction algorithms would also likely deliver improved overall accuracy, as discussed above. Specifically, perhaps incorporation of machine learning in the answer extraction algorithm as well as the passage selection algorithm may prove beneficial. This is a tack that will be pursued in future work.

Now, consider the following problem from the CFE test set:

\blockquote{True or False: Green, an agent for the White Corporation, acting within the scope of his duties and for the benefit of White, committed an act of fraud. According to the Sentencing Guidelines effective in 1991, White cannot be held criminally liable for Green's actions. \cite{acfe_study_package_2011}}

This is a true/false question, a type that is not specifically targeted in any of the first three versions of the CFE agent. The reason for that is obvious – this is a tough problem for the CFE Agent using the algorithms of Versions 1, 2, and 3 where the algorithms are looking for the juxtaposition of key words of the question stem and those of the various answer options within the CFE Manual. In a true/false problem, however, answer options are of no help, leaving the agent with only the question stem from which it must discern whether the statement is valid.  Using proximity of key words will likely not work here as it is likely that such questions with keywords richly represented in a particular section are as likely to be invalid as valid. 

An algorithm that is based on a semantic understanding of the problem would certainly appear useful, here. But for that, we'd need the tools for translating natural language assertions to ones expressed in a formal language, where in our case, we'd recommend the $\mathcal{DCEC}^\ast$ due to its built-in support for modeling the cognitive states and interactions typical of agents in financial fraud scenarios. There are two possible approaches for tackling this task. The first approach to the translation problem is a computational semantics approach based on Blackburn and Bos \cite{blackburn_2005_representation_ch1, blackburn_2005_representation_ch2, blackburn_2005_representation_ch3, blackburn_2005_representation_ch4, blackburn_2005_representation_ch5, blackburn_2005_representation_ch6} using Prolog \cite{
blackburn_2006_prolog_ch7,
blackburn_2006_prolog_ch8,
blackburn_2006_prolog_ch9,
blackburn_2006_prolog_ch10,
blackburn_2006_prolog_ch11,
blackburn_2006_prolog_ch12}, wherein the authors explain an approach to the representation of natural language assertions in first-order logic ($\mathcal{FOL}$) \cite{blackburn_2005_representation_ch1} using the lambda calculus \cite{blackburn_2005_representation_ch2} and inference \cite{blackburn_2005_representation_ch4, blackburn_2005_representation_ch5} from these $\mathcal{FOL}$ assertions. In the approach we propose here, however, the target representation language would be the $\mathcal{DCEC}^\ast$, not $\mathcal{FOL}$.  A second approach would involve parsing the sentences first, using a probabilistic parser \cite{martin_2000_speech_ch14}, such as OpenNLP (https://opennlp.apache.org) or the Stanford Parser (http://nlp.stanford.edu/software/lex-parser.shtml), and then translating the parse structures into assertions expressed in the $\mathcal{DCEC}^\ast$.  The idea here is that both the natural language utterances and the parse structures serve as inputs to the computational semantics model that produces the output assertions in the $\mathcal{DCEC}^\ast$ from which inferences can be drawn using a theorem prover.  Having parse structures as inputs may be helpful in better capturing the semantics of the utterance in the semantics model. 

As a final thought, we believe that any comprehensive QA system that provides both answers with a high degree of accuracy along with justifications, a holistic approach would be required – one that leverages extended versions of the machine learning/information retrieval approaches in Versions 2 and 3 of the agent as well as the semantic approach of Version 4. How might that be accomplished? Depending on the type of question, this holistic approach could take on different forms. For true/false questions, as we have seen, machine learning approaches may be of limited use, and thus an approach more centered around semantic computation is required.  However, for definition questions, a high level approach might be to leverage the machine learning approach to arrive at an answer, and then use that answer to engineer the justification using a semantic approach.  The idea here is to utilize the machine learning approaches for finding the answers and then theorem proving for explaining them. For other types of questions, still yet a different blend of these approaches may be required.