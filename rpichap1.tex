%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%                                                                 %
%                            CHAPTER ONE  - Introduction and Overview                        %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
 
\chapter{Introduction and Overview}

\blfootnote{Portions of this chapter previously appeared as: \nobibentry{johnson2015cfeagent}.} 

\section{Background}
Question Answer (QA) \cite{martin_2000_speech_ch23, radev2000ranking} is a prominent and growing sub-field of natural language processing, and of the larger field of AI \cite{russell_norvig_2010_ch1}.  QA systems \cite{radev2000ranking} consist of agents that provide responses (correct ones, hopefully) to questions posed in natural language. These systems are typically based on one of two approaches: an information-retrieval approach \cite{manning_2008_introduction_ch1,manning_2008_introduction_ch2,manning_2008_introduction_ch4,manning_2008_introduction_ch6} or a knowledge-based approach \cite{brachman_2004_knowledge_ch3,brachman_2004_knowledge_ch4,brachman_2004_knowledge_ch5}.  Some of the most successful systems as of late, however, utilize a blend of these two approaches, such as IBM's Watson \cite{ferrucci2010building}.  A knowledge-based approach offers the advantages of allowing automated reasoning, and thus, justifications for answers.  But the knowledge bases involved have been manually curated by domain experts and knowledge engineers -- a time-consuming, labor intensive endeavor.  An information-retrieval approach, where candidate answers are screened from passages retrieved from documents retrieved from a core information-retrieval (IR) system, are designed without the need to manually curate a knowledge base, but typically offer no justification of answers.  In fact, these systems have no understanding of the reasons for the answers they put forth, but instead typically provide answers based on statistical approaches that are utilized in black-box fashion, giving the user no insight into the rationale for responses.



\section{Goals}

This dissertation explores various approaches for developing an intelligent agent in a particular domain, fraud detection, in the context of an artificial-intelligence framework, called \textit{psychometric artificial intelligence}, or, more commonly, \textit{psychometric AI}.  \textit{Psychometric AI} focuses on the creation of agents that can successfully pass tests of mental ability, and offers a number of benefits, including a well-defined domain, a built-in measure for quantifying the efficacy of the agent in the form of a test score, and a rich environment for deploying various forms of AI, including machine learning, natural language processing, computer vision, et cetera.

This dissertation attempts to advance the field of AI in three ways:  First, introduce and measure the performance of new algorithms in the AI subfield of question-answer (QA) that provide justifications for their reasoning.  Second,  provide a comparative analysis of these approaches in terms of their accuracy, relative complexity, and their ability to provide justification for their reasoning by assessing three versions of the CFE Agent -- Versions 1, 2, and 3 -- in which these approaches are step-wise implemented.  By doing so we address an important question in QA -- What are the various marginal costs/benefits associated with state-of-the-art AI approaches in terms of the competing priorities listed above. And third, lay the groundwork for the rigorous study of fraud detection using the formal representation and reasoning approach demonstrated in Version 4.  

% removed 07/22/2016
%The work we present here seeks to advance the state of the art of QA by exploring this area within the context of developing an agent designed to correctly answer multiple-choice questions covering the domain of fraud detection.   Also, we seek to lay the groundwork for the rigorous study of fraud detection using the formal representation and reasoning approach demonstrated in Version 4.  

% removed 07/13/2016
%In short, then, the goal of this thesis is threefold:  First, introduce the use of psychometric AI \cite{psychoai.ijcai03} for researching methods in QA, where psychometric AI, here, is the development and validation of intelligent systems by applying their algorithms to taking multiple choice tests.  Second, explore techniques by which the agent can answer questions while providing reasoning for its answers, and specifically, using proof-based techniques.  Third, lay a foundation for the rigorous science of fraud detection.  Finally, after having explored the various techniques presented in this thesis, make recommendations about future research in the field of QA.  In particular, we consider ways in which semantic approaches can be utilized in conjunction with the techniques presented here for developing knowledge-bases to advance the art of QA in the semantic direction.

\section{The Intelligent Agent Defined, For This Project}
In an effort to further detail the goals listed above, the definition of intelligent agent is made more explicit, here, as one capable not only of making intelligent decisions, but also of providing a justification for those decisions.  The ideal for a justification is one that is proof-based as exemplified in \cite{bringsjord2015logicist} and \cite{johnson2014three}.  Just shy of this ideal is a different approach that shows the premises for each of its conclusions, as in justification-based truth maintenance systems, (JTMS), as described in \cite{russell_norvig_2010_sect12.6}. Finally, absent these, we'd like at least some explanation from the agent for its reasoning (as we'll discuss for Versions 2 and 3).  This is to be contrasted with agents of other AI systems where decisions are provided typically without any justifications.\footnote{For example, in a facial recognition system, the agent typically matches identities to faces in photographs without any explicit explanation for its reasoning.} 

%This type of agent, which is the goal for this project, will be termed a ``reasoning agent".  It should also be mentioned the reasoning agent provides justifications in situations where there is uncertainty of outcome (which is typically the case for the domain in this project).

%# JOE: It's important to think about /how/ the agent will show its
%# reasoning.  The IBM folks, wrt to Watson, claimed that Watson did
%# show its reasoning.  But that's a very, very big stretch.  I think
%# the idea should be that the reasoning must be naturally sequential,
%# which means argument- or proof-based.  What if you coined a label
%# for the type of agent that is your focus?  Career-wise, I think that
%# would make a lot of sense.  //S
%
%# SELMER: understood.  I will keep this in mind as the dissertation 
%# research moves forward.

\section{An Overview of the Approach}

The overarching idea is to develop successively more sophisticated agents -- starting with a naive agent that uses only shallow techniques that largely leverage question features with patterns discovered from exploring a training set.  In Version 2, we elevate the level of sophistication of the agent, but still limit it to fairly shallow techniques, by basing answers on information-retrieval-based techniques that incorporate sophisticated query-generation and document-collection-development techniques.  In Version 3, we explore machine learning approaches for a passage selection algorithm.  And finally, Version 4 consists of a  demonstration of the behavior of an agent, which, if it were implemented, would use deep, formal, semantic techniques to answer questions about a subdomain of fraud detection -- doctor shopping.

% 2016-07-05 remove this section as it is no longer valid for the thesis.
%\subsection{Generation of KB assertions from Text Corpora and MCQs}

%In version 3, where as mentioned, the agent utilizes a knowledge-base to answer questions,
%we'll look at the generation of assertions from both the text corpus (the CFE Manual, discussed below),
%and from other multiple-choice questions that overlap in the problem domain.  By overlap, we mean
%that the question stem and correct answer of one question serve as the basis for assertions from which inferences can be made about the answer to a second, separate question.  Unfortunately, as we'll discuss later, the sparsity of the training set and test set were such that no such overlap could be found, so
%engineered test-battery questions were created to demonstrate this approach.

\subsection{Reasoning with Uncertainty}

In order to cover the nondeterminism inherent when extracting information using NLP techniques, the agent incorporates uncertainty into its decisions.  As we'll see in Version 1 and Version 2, the agent quantifies uncertainty by analysis of its various algorithms on the training set, and then utilizes this information to optimize its accuracy on the test set.  It also quantifies uncertainty in some intermediate steps of its algorithms, such as in the paragraph selection algorithm in Version 3.

\subsection{Verification of the Agent via Test-Taking}

Algorithm performance is assessed by measuring accuracy on multiple-choice questions.  In this project, the domain of fraud detection is used, for which there is a well-developed, effective industry for testing subjects on knowledge in this domain.  The principal organization responsible for this and its examination process are described below.

\subsubsection{The ACFE and the CFE Exam}


The ACFE \cite{acfe} describes itself on its website as ``the world's largest anti-fraud organization and premier provider of anti-fraud training and education'' \cite{acfe}.  Generally speaking, in order to become a readily employable expert in the field of fraud detection, certification by this organization is required.  (Among notable certified members of the ACFE is Harry Markopolos, the American forensic accountant who achieved fame by being the first to have uncovered the ponzi scheme perpetrated by Bernard Madoff and desperately tried to warn government securities officials years before the scam collapsed in the midst of the 2008 financial crisis \cite{markopolos2010}.)  The ACFE has well-defined requirements for becoming certified, based on a point system that considers a combination of professional experience and academic credentials.  However, the CFE exam is the credentialing centerpiece for the ACFE, and the details of this exam are described briefly below.

The CFE Exam is a computer-based exam.  The mechanics for preparing for and taking the exam begins with downloading a software package from the prep course page of the ACFE website.  This package includes the exam software, the Fraud Examiners Manual (on which the test is based), and a self-study application consisting of a battery of sample test questions, a complete practice exam, and tools for monitoring progress.  

The CFE exam consists of 4 sections, listed below:

\begin{enumerate}
\item Financial Transactions and Fraud Schemes 
\item Law 
\item Investigation
\item Fraud Prevention and Deterrence.  
\end{enumerate}

Each section consists of 125 multiple-choice and true-false questions.  The candidate is limited to 75 seconds to complete each question and a maximum total allocated time of 2.6 hours to complete each section.  Each CFE Exam section is taken separately.  The timing for each section is subject to the candidateâ€™s discretion.  However, all four sections of the exam must be completed and submitted to the ACFE for grading within a 30-day period.

\subsubsection{CFE Manual}

The CFE Manual is the text corpus on which all of the questions of the CFE Exam are based.  Each question includes a section heading that loosely maps to an individual section within the manual.  However, these sections are rough-grained, often 20--100 pages.  

\section{Code Resources}

The code (written in Java \cite{java_software} and R \cite{r_software}) for the CFE Agent and its supporting software components are available at https://github.com/jjohnson346/cfe-exam-agent, (Accessed on:  July 21, 2016).








