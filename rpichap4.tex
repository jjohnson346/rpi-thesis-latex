%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%                                                                 %
%                            CHAPTER FOUR   - A More Sophisticated Agent- Version 2            %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
 
\chapter{Version 2 -- A More Intelligent Agent}

In this chapter, we describe Version 2 of the CFE Agent, enhanced with improved algorithms using information retrieval for locating the relevant section(s) of the CFE Manual for each question.\footnote{For an overview of the topic of Information Retrieval, see Appendix \ref{app:information_retrieval}.}  Version 1 of the CFE Agent is limited to coarse-grained sections as defined by the question sections of the CFE Exam and Manual.  Unfortunately, these sections of the manual are effectively so large, it's difficult for the agent to drill down to the text \emph{directly} related to a given question.  For intellectual-property-theft questions, for example, the ``Theft of Intellectual Property" section of the manual is 89 pages long.  For any question on this topic, pulling the entire section of the CFE manual will assure that the answer sought lies somewhere within our target document.  But with 89 pages of text in the section, we might still get the answer wrong.  Version 2 refines the filtering algorithm to help avoid this problem.



This chapter is laid out as follows:  First, we discuss the method for breaking up the CFE manual into fine grained sections, leveraging the table of contents and text metadata.  Next, we explore Lucene \cite{lucene}, an Apache software \cite{apache} component that provides information retrieval functionality, which was used in the development of Version 2.  Next, we briefly describe some support tools there were built to facilitate algorithm analysis and refinement in this iteration.  And finally, we discuss the algorithms of Version 2 and their performance.  

One note before proceeding:  The terms, ``training set" and ``test set" are used throughout this chapter in reference to the battery of questions against which the algorithms discussed below were applied.  Indeed, we talk repeatedly about testing our algorithms against the training set, specifically. It is important to note here that ``training set" is a term of convenience in this chapter, and not one that should be taken to suggest a set for which we've optimized parameters vis \`a vis machine learning.  In fact, the algorithms we discuss in this chapter involve no training, per se, and so, it is perfectly valid to measure or test the performance of each of these algorithms against the ``training set".\footnote{In chapter 5, however, we \emph{do} employ machine learning, and the training set and test set are used according to their conventional definition.}  Lastly, it should be mentioned that at the end of this chapter, we use the performance of each algorithm on the questions of the training set to determine the optimal algorithm to use on each question in the test set in order to measure overall performance of Version 2 of the agent.

Finally, before moving any further, during the development of Version 2, a deeper review of the definition-type questions was in order.  Some questions were re-classified according to finer grained criteria, including the following:  Nine questions were reclassified as ``I, II, III, and IV" type.  Six questions for which ``Any of the Above" was an option were re-classified as ``All of the Above" type.  There were 26 questions whose answers weren't derived from the Fraud Examiners Manual but instead from a different text, The Corporate Fraud Handbook.  Finally, five questions were re-classified as ``NOT" questions.  After reclassifying these questions, we were left with 150 definition questions in our training set upon which to base our algorithm performance analysis, reduced from our original 196 questions used for Version 1.


\section{Transforming the CFE Manual into a Document Collection}

The CFE Manual is structured as a text book. As such, it is structured hierarchically, as most textbooks are, complete with features embedded in the text that make this hierarchy apparent.  The most obvious feature is the table of contents (TOC) and headings embedded in the text for each of the chapters, sections, subsections, and so on.  In fact, the CFE Manual has a number of tables of contents, including a main table of contents, at the front of the manual, and a set of area-specific TOCs - one for each of the major test areas - Financial Transactions and Fraud Schemes, Law, Investigation, and Fraud Prevention and Deterrence.  Fig.~\ref{fig:cfe_manual_toc} shows a section of the TOC relating to Financial Statement Analysis, a topic contained in the area of Financial Transactions and Fraud Schemes.  The summary TOC combined with the area-specific TOCs combined with text features (capitalized sub-sub-sub section titles within the text itself) were all used to programmatically break up the manual into a hierarchical structure of documents.  Fig.~\ref{fig:document_collection} shows a portion of this document structure, where on the left we see the Bankruptcy Fraud subsection, a subtopic of Financial Transactions and Fraud Schemes, and the breakdown of documents, each of which named according to a numeric identifier and a title corresponding to a title for the subsection, along with indentation showing its level in the document tree.  On the right side, we see the contents of one of the documents covering the subtopic of Bankruptcy Court.  Notice that in each document we have not only the text of the section but also a a title field (Bankruptcy Court), a question section field (Financial Transactions and Fraud Schemes), a path giving the sequence of section titles starting from the root node of the CFE manual hierarchy to the current document, and finally, the stemmed contents of the document, using the Porter Stemmer algorithm.  All of these elements were compiled for each document as possible inputs to algorithms developed downstream for answering questions.  

\section{Lucene -- A Tool for Information Retrieval}

Lucene \cite{lucene} is a highly popular Apache software product that implements IR using a combination of Boolean Retrieval and the Vector Space Model (VSM) \cite{McCandless:2010:LAS:1893016_ch1,McCandless:2010:LAS:1893016_ch2,McCandless:2010:LAS:1893016_ch3,McCandless:2010:LAS:1893016_ch4}.  First, it narrows the document set using boolean retrieval, and then it ranks the remaining documents using VSM.  The algorithms described below were implemented using this tool.  

\begin{figure}
\centering
\vspace{1.0in}
\framebox{\includegraphics[width=150mm]{cfe_manual_toc.png}}
\caption{A section of the table of contents from the CFE Manual}
\label{fig:cfe_manual_toc}
\end{figure}

\begin{figure}
\centering
\vspace{1.0in}
\framebox{\includegraphics[width=150mm]{document_collection.png}}
\caption{The CFE Manual as a Document Collection}
\label{fig:document_collection}
\end{figure}

However, before implementing any algorithms, the document collection into which the CFE manual was decomposed was indexed using the Lucene software.  Indexing is the process of essentially creating files containing the underlying data stuctures required by Lucene's combined boolean retrieval/VSM retrieval algorithm, including an inverted index for the collection and document vectorization data.  In fact, a Lucene index was created for the documents of each question section.   For a given question, Lucene processes a query against the index for the corresponding question section, returning a set of documents from which an answer is drawn.  When implementing the QA algorithms discussed below, the first step to any of these algorithms was locating the proper index within which to search for relevant documents, based on the exam section/question section that corresponds to the question at hand.  Fig~\ref{fig:lucene_indexes} shows a portion of the Lucene indexes created by this process.  Notice that for each question section within each exam section, there are three binary files created by the Lucene indexer component that contain the inverted index and document vectorization information required for the query processing component to be used in the algorithms discussed below.

One other thing of note is that as part of this process, the contents field and the title field were stemmed according to the Porter Stemmer algorithm.  Stemming is a form of semantic normalization, where words offering different senses of the same semantic unit are transformed so that they are treated as equivalent during the document scoring computation in the IR process.  For example, different words for run - run, ran, running - are all considered semantically equivalent as a result of stemming.

\begin{figure}
\centering
\vspace{1.0in}
\framebox{\includegraphics[width=125mm, height=200mm]{lucene_indexes.png}}
\caption{Lucene Indexes for a Portion of the Question Sections}
\label{fig:lucene_indexes}
\end{figure}



\section{Analysis Tools for Algorithm Development}

As outlined in prior chapters, the goal of the CFE agent is to answer questions correctly while providing justification for those answers.  As algorithms were developed toward this end, and in particular, as we attempted to refine the accuracy of the agent by making its search functionality more fine-grained, it was determined early in the process that one of the most critical pieces of information was to understand how to target each type of question - What features for a given type of question could be exploited when searching for an answer?  Specifically, how does the answer present itself in the manual to a question of a given type?  Is it contained in a single document, or multiple documents?  Are terms found in the options commonly found in the contents of the document, or are they found in the title?  Depending on the answer, how often is that the case?  Is it always true, or only sometimes?  Tools that aided in this investigation were critical to the development of ``smarter'' algorithms.  

At a macro level, the profiler component, developed for Version 1 of the agent provided an initial analysis tool.  As discussed, it supplied a breakdown of question by macro-features, as well as the success rate of the initial algorithms created for that version.  And here in Version 2, the profiler would be used again.  However, analysis at a greater level of detail was needed.

One component, called the Question Server, was created to at least partially meet this need.  It poses to the agent only questions of a single profile, (all definition questions, or long answer questions, and so on).  This provided a means for zero-ing in on each question type in isolation, which was key in algorithm development.  Fig.~\ref{fig:question_server} shows output from the Question Server for definition/NOT questions, questions whose options are a small number of words (and are thus, typically relate to a definition of a concept), and contain the term, ``not" in the stem, implying the task is to identify the ``odd man out" among the options.

A second component, called the Algorithm Tester, was created on the shoulders of the Question Server, which would demonstate the behavior of each algorithm as it was applied to each question of a given type.  This, too, was instrumental in algorithm development.

\begin{figure}
\centering
\vspace{1.0in}
\framebox{\includegraphics[width=125mm, height=110mm]{question_server.png}}
\caption{Question Server Component Targeting Definition/NOT Questions}
\label{fig:question_server}
\end{figure}


\section{Algorithms for Version 2}

This section describes the algorithms implemented for Version 2 of the CFE Agent.  These algorithms rely heavily on IR, as implemented in Lucene.  They also tend to each target a specific question type, in particular, the definition questions - that is, those questions in which each of the options is only a short phrase (consisting of four words or less), and which thus are thought to be most likely the kind of question in which the stem contains a phrase that defines a given concept and the examinee must choose the correct concept from among the four options to which that phrase corresponds.  Given that a large portion of the questions in the training and test sets are some form of this type, more refined algorithms that target this type of question appeared to be a wise choice for where to focus our development efforts.

\subsection{Concept Match Version 1}

Concept Match Version 1 leverages IR on both the question stem and on each of the options in order to determine the one that fits best with the stem.  The essence of the algorithm is to first, conduct an IR query against the document collection based on the terms of the question stem, then, conduct a query based on the terms of each of the question options, and finally, select the option whose return set has the ``best overlap" with the return set for the question stem.  How do we define ``best overlap"?  For the purpose of this algorithm, a return set with the ``best overlap" is the return set whose intersection with the stem query return set contains the document with the highest score relative to the intersection sets for other return sets.

Consider an example.  Fig.~\ref{fig:concept_match_v1_example} shows a Bankruptcy Fraud definition question.  (Although the criterion for being a definition question has only the naive requirement that all options be no more than 4 words, this example demonstrates how this criterion is often sufficient for properly categorizing this type of question, since this question really \emph{is} a definition-type question.)  The justification by the agent shows its reasoning - First, it shows the return set for each option of the four options in the question, including for each document its title, id (automatically assigned by the Lucene indexing component), and score.  (This example is particularly convenient as each option has the simplifying characteristic of at most one return document in its return set.  This is not always the case.)  Next, the agent shows the return set for the query based on its stem, ``a person who holds a perfected security interest against a person filing bankruptcy".  The return set is sorted by decreasing score.  Now, the option, ``Secured creditor" has a result set that includes the ``SECURED CREDITOR" document (it's capitalized because that's the way it appears in the text for the manual), which possesses a higher score in the question stem return set than any of the other documents in the return sets for the other options.  (In this particular case, it's also \emph{the} high scoring document in the question stem return set, although this fact is not a requirement of the algorithm.)  This means that the ``best overlap" is the ``SECURED CREDITOR" document, and as a result, the agent picks option, ``a) Secured creditor", the correct answer.

Some further details about the mechanics of this algorithm should also be mentioned here.  The algorithm conducts a query against the document collection based on the stem to return a collection of 10 documents (this number chosen arbitrarily) relating to the question.  This query is conducted against the \emph{contents} field of each document.  (Note that as alluded to previously, each document consists of four fields - a title field, a contents field, a question section field, and a path field.)  This means that when retrieving documents in response to the query, the terms in the contents field, exclusively, are used to determine the relevance of the document to that query.  The document's title and path are not considered (at least not in this algorithm).  It should also be noted that before conducting the query, functional phrases including ``is referred to as", ``are referred to as", ``which of the following", and so on, were removed from the stem as they do not offer semantic information about the nature of the question.  Also, the words of the question stem were stemmed just as the terms in the contents field of each document were during the indexing process.  This is necessary in order for the scoring to work properly.  

As for the queries based on each of the question options, they are executed against the \emph{title} field of each of the documents.  So, in order for a document to be found to match well to an option, it must have one or more terms in its \emph{title} in common with those of the option.  This greatly narrows the range of documents that produce high ranked hits, and this was by design.  It was noticed that on more than a few questions, the options map nicely to subsections whose titles align closely with the options.  Unfortunately, however, in the event the question options do not have sibling sections in the manual, this algorithm does not perform well.


\begin{figure}
\centering
\vspace{1.0in}
\framebox{\includegraphics[width=160mm, height=150mm]{concept_match_v1_example.png}}
\caption{Concept Match V1 Example}
\label{fig:concept_match_v1_example}
\end{figure}


\subsubsection{Agent Justification for Selected Answer}

Notice, in the above example, that the agent shows its reasoning for its argument.  By giving the scored return set for the question stem, and for each option, the agent provides the detail for its argument for the option it selects.  It is clear from the detailed output that among the documents in the returns set for the question stem query and among the set of documents in the return sets for the question options queries, there are two documents that appear in both - SECURED CREDITORS, (id = 9) and UNSECURED CREDITORS (id = 10).  Since the SECURED CREDITORS document earns a higher vectorization match score of 0.3375, the reasoning for selecting the Secured creditors, option a, is clear.

\subsubsection{Concept Match V1 Performance}

Finally, we look at the performance of the Concept Match Version 1 algorithm.  Fig.~\ref{fig:concept_match_v1_training_set_results_def} shows output from the algorithm tester tool described above, showing that among the 150 questions in the training set that were classified as strictly definition questions,\footnote{There were other questions that met the criterion for the definition question type, but were classified in different but related categories, such as the definition/not category, or definition/except category.} this algorithm correctly answered 56 questions, or 37.3\% -- not an outstanding rate.  In fact it is lower than the rate of the Version 1's Max Frequency algorithm\footnote{See  Fig.~\ref{fig:version_2_training_set_performance}, row for profile, 4.} on the same questions. However, these results also show that for 46 questions this algorithm produced no answer at all.  This was because the laser-focused question option queries against the title field of the documents in some cases returned 0 documents for \emph{all} options, thereby causing the algorithm to fail.  Fig.~\ref{fig:concept_match_v1_financial_statement_fraud_9} shows an example of this scenario.  With no documents in the set of return sets for the option queries, the algorithm has no other choice than to return -1, (for no option selected).  In the next algorithm, Concept Match V2, however, this issue is addressed.


\begin{figure}
\centering
\vspace{1.0in}
\framebox{\includegraphics[width=110mm, height=16mm]{concept_match_v1_training_set_results_def.png}}
\caption{Performance of Concept Match V1 on Definition Questions}
\label{fig:concept_match_v1_training_set_results_def}
\end{figure}

\begin{figure}
\centering
\vspace{1.0in}
\framebox{\includegraphics[width=125mm, height=110mm]{concept_match_v1_financial_statement_fraud_9.png}}
\caption{Concept Match V1: An Example Where No Docs Are Returned for Options}
\label{fig:concept_match_v1_financial_statement_fraud_9}
\end{figure}


\subsection{Concept Match Version 2}

Concept Match Version 2 extends Concept Match V1 by addressing two major concerns.  The first is the situation outlined above in which for the query options no documents are returned because of the tight focus of the queries on the title field.  In Concept Match V2, if this scenario occurs, the options returns sets are rebuilt, but instead of searching on the title fields, the the search is performed on the contents fields.  This loosens the focus of the option queries, promoting the likelihood of non-empty return sets.

Fig.~\ref{fig:concept_match_v1_financial_statement_fraud_9} shows shows another question for which Concept Match V1 fails due to empty return sets for the option queries.
The two figures, Fig.~\ref{fig:concept_match_v2_financial_statement_fraud_9_1} and Fig.~\ref{fig:concept_match_v2_financial_statement_fraud_9_2}, show the output for Concept Match V2 for the same question. Whereas the V1 algorithm gives up and returns -1, V2 redoubles its effort and re-issues the same option queries against the contents field, resulting in the correct answer.

\begin{figure}
\centering
\vspace{1.0in}
\framebox{\includegraphics[width=125mm, height=125mm]{concept_match_v2_financial_statement_fraud_9_1.png}}
\caption{Concept Match V2: Fixing the No Docs in Option Queries Return Sets Problem, Part 1}
\label{fig:concept_match_v2_financial_statement_fraud_9_1}
\end{figure}

\begin{figure}
\centering
\vspace{1.0in}
\framebox{\includegraphics[width=125mm, height=125mm]{concept_match_v2_financial_statement_fraud_9_2.png}}
\caption{Concept Match V2: Fixing the No Docs in Option Queries Return Sets Problem, Part 2}
\label{fig:concept_match_v2_financial_statement_fraud_9_2}
\end{figure}

The second major concern this algorithm addresses is demonstrated by the following example shown in Fig.~\ref{fig:concept_match_v1_wrong_option_doc}.  The highest scoring document common to both the question stem query return set and option query return sets, (and importantly, also the document containing the correct answer) is docID, 38, ``The Business Profile Analysis".  Unfortunately, this document is also in the return sets for \emph{two} options -- option b, ``Preparing the business profile", and option c,  ``Preparing the vertical analysis", making the correct choice ambiguous.  But since Concept Match V1 loads these documents into the conceptDocs hash map\footnote{The conceptDocs hash map is used by the concept match algorithms to find the highest scoring document common to both the stem query result set and the option query result sets.  This hash map contains a collection of key/value pairs, where the key stores a docID and the value stores an option id, (0, 1, 2, or 3).  For a given docID/option pair, the docID is the document ID for a document found in the return set for one of the options, whose id is given in the value field.  The line in the output that starts with ``conceptDocs:" shows the contents of this map.  In Fig.~\ref{fig:concept_match_v1_wrong_option_doc}, we see that docID, 49, is returned for option 1 (b), docID, 3, is returned for option 1 (b), docID, 38, is returned for option 2 (c), and so on.} in order by option, docID, 38, is \emph{initially} mapped by the algorithm to option b, the correct answer, but is \emph{subsequenty} mapped to option c, the incorrect answer.  We can see this association in the display of the contents of the conceptDocs hash map, in which the key/value association, 38=2, signifies that docID, 38, is associated with option 2 (that is, option c).\footnote{Option ids are 0-based, so option 0 corresponds to option a, 1 to b, 2 to c, and 3 to d.}  As a result, the agent gets this question wrong.

\begin{figure}
\centering
\vspace{1.0in}
\framebox{\includegraphics[width=125mm, height=125mm]{concept_match_v1_wrong_option_doc.png}}
\caption{Concept Match V1: An Example Where A Document is Returned/Ranked for More than One Option}
\label{fig:concept_match_v1_wrong_option_doc}
\end{figure}

Concept Match Version 2 corrects this problem by recognizing a situation in which a document is included in multiple option return sets.  In this case, the algorithm maps the document to the option for which that document earned the highest rank score.\footnote{The Lucene scoring algorithm is normalized so that scores for documents from different queries may be compared.}  Explaining this a bit more rigorously, for each concept document, $D$, with score, $S$, with respect to option $O$, if there is already an entry in the map for which the there is key/value pair, $D=(O',S')$, where $O'$ is a different question option and $S'$ is the score for $D$ with respect to $O'$, then scores, $S$ and $S'$ are compared, such that the option whose score is greater is chosen for $D$. If $S' > S$, then the entry, $D = (O',S')$, is left as is in the map; otherwise, it is overwritten with $D = (O,S)$.

For the ``Bribery and Corruption" example discussed below, we see in Fig.~\ref{fig:concept_match_v2_multiple_concept_docs} that Concept Match V2 properly associates the document, ``The Business Profile Analysis" (id = 38), with the correct option, ``b) Preparing the business profile".  The printout of the conceptDocs hash map shows the correct key/value association, 38=1, signifying that document 38 is now associated with option b, as opposed to option c.


\begin{figure}
\centering
\vspace{1.0in}
\framebox{\includegraphics[width=125mm, height=125mm]{concept_match_v2_multiple_concept_docs.png}}
\caption{Addressing the Problem of Multiple Options for a Document}
\label{fig:concept_match_v2_multiple_concept_docs}
\end{figure}

\subsubsection{Concept Match V2 Performance}

Next, we look at the performance of the Concept Match Version 2 algorithm on the same definition questions that were used to test Concept Match V1.  Fig.~\ref{fig:concept_match_v2_training_set_results_def} displays output from the algorithm tester showing Concept Match V2 correctly answers 101 questions of the of the 150 definition questions, resulting in a score of 67.3\% -- a dramatic improvement over Concept Match V1.  We also see that this algorithm has a much lower population of unanswered questions, (3 instead of 46), as a result of the fallback query measure incorporated into V2.



\begin{figure}
\centering
\vspace{1.0in}
\framebox{\includegraphics[width=110mm, height=16mm]{concept_match_v2_training_set_results_def.png}}
\caption{Performance of Concept Match V2 on Definition Questions}
\label{fig:concept_match_v2_training_set_results_def}
\end{figure}

Fig.~\ref{fig:concept_match_v2_hypothesis_test} shows we can reject the null hypothesis, H0, in favor of the alternative Hypothesis, Ha, that Concept Match V2 offers improved performance over Version 1's Max Frequency algorithm, whose accuracy is 48\%\footnote{see Fig.~\ref{fig:version_2_training_set_performance} to view the accuracy of Max Frequency on the reclassified training set of 150 definition questions.} on the same 150 questions, at the 99\% significance level.

\begin{figure}
\centering
\vspace{1.0in}
\framebox{\includegraphics[width=75mm, height=60mm]{concept_match_v2_hypothesis_test.png}}
\caption{Concept Match V2 vs. Max Frequency Hypothesis Test on Definition Questions}
\label{fig:concept_match_v2_hypothesis_test}
\end{figure}


\subsection{Concept Match Version 3}

Concept Match V3 attempts to build on Concept Match V2 by leveraging a behavior that was noticed in the results of the Lucene search results of the stem query.  
During development, it was noticed in a plurality of cases that return sets for the question-stem query were headlined by a document whose score was head-and-shoulders above the rest in the return set, sometimes by a factor of three or more.  In these cases, it was common this document was, in fact, the correct one, containing the answer to the question.  So, when we have such a document, hereafter referred to as a ``premier document", the algorithm should focus on finding the option most closely associated with that premier document.  Concept Match V3 implements this approach in questions who are found to have a premier document by choosing the option for which the premier document scores highest.  If no option queries based on the title field yield the premier document, then the algorithm repeats the option queries against the contents field.  If the premier document \emph{still} does not appear in any result set, the algorithm returns -1, representing no selection.

Consider the example in Fig.~\ref{fig:concept_match_v3_example_part_1} and Fig.~\ref{fig:concept_match_v3_example_part_2}.  The question stem query for this ``Criminal Prosecutions for Fraud" question returns a result set in which the ``Arraignment" document, (id=12), with a score of 0.4654 outscores the next place document, ``Sentencing" (id=45) by a facor of more then 2.5x.  The algorithm, therefore,  categorizes this document as a premier document, and thus approaches the option selection process by attempting to find the option whose query result ranks this document higher than that for any other option.  In this example, we see that for the option queries based on the title field, the result sets are thin and there's no match to the premier document.  So, the algorithm re-issues the option queries, this time against the contents field of each document in the collection.  In Fig.~\ref{fig:concept_match_v3_example_part_2}, we see that this approach prevails -- the result set for the correct answer, option b, the Alford plea, includes the Arraignment document.  Further, we see that although this document appears in the result sets for other options' queries, it scores highest for the Alford plea option.

\begin{figure}
\centering
\vspace{0.75in}
\framebox{\includegraphics[width=125mm, height=125mm]{concept_match_v3_example_part_1.png}}
\caption{Concept Match V3 Example - Part 1}
\label{fig:concept_match_v3_example_part_1}
\end{figure}

\begin{figure}
\centering
\vspace{0.75in}
\framebox{\includegraphics[width=125mm, height=125mm]{concept_match_v3_example_part_2.png}}
\caption{Concept Match V3 Example - Part 2}
\label{fig:concept_match_v3_example_part_2}
\end{figure}

Fig.~\ref{fig:concept_match_v3_example_document} shows the ``Arraignment" document.  It provides a couple of key insights as to the behavior of the algorithm on this particular question.  First, we notice that the answer to the question is provided on lines 39 through 42.  The brief segment shown here concerning a description of the Alford plea suggests (and, in fact, it turns out to be the case upon further checking) that there is no document in the collection specifically dedicated to (and whose title field would be) the Alford plea.  Second, a cursory examination of this document reveals that there are number of occurrences of the term, ``plea" throughout the document which would explain why this document appears in the result set for not only the Alford plea option, but also for the Brady plea, Johnson plea, and Katz plea options.  However, the name, ``Alford" is the only one of these names mentioned in this document, and this explains why for the Alford plea option, this document scores significantly higher than for any other option.

\begin{figure}
\centering
\vspace{0.75in}
\framebox{\includegraphics[width=125mm, height=125mm]{concept_match_v3_example_document.png}}
\caption{The Arraignment Document}
\label{fig:concept_match_v3_example_document}
\end{figure}


\subsubsection{Concept Match V3 Performance}

Fig.~\ref{fig:concept_match_v3_training_set_performance} shows the performance of the Concept Match V3 algorithm on the 150 definition-type questions of the training set.  It shows that V3 improves on V2 slightly, getting 105 question correct compared with V2's 101.  However, this improvement is not significant enough to be statistically significant at the 99\% level, as the figure~\ref{fig:concept_match_v3_hypothesis_test} shows.  Nonetheless, the fact that V3 outpaces V2 means that the CFE agent will use V3 over V2 when confronted with a definition-type question, (and, as we'll see later, the agent will use this algorithm on other types of questions as well).  And finally, lest we forget, compared with Version 1 of the agent, this algorithm extends the gains we achieved with Concept Match V2 that were, themselves, found to be statistically significant relative to Version 1 of the CFE agent.

\begin{figure}
\centering
\vspace{0.75in}
\framebox{\includegraphics[width=110mm, height=16mm]{concept_match_v3_training_set_performance.png}}
\caption{Performance of Concept Match V3 on Training Set}
\label{fig:concept_match_v3_training_set_performance}
\end{figure}



\begin{figure}
\centering
\vspace{0.75in}
\framebox{\includegraphics[width=75mm, height=60mm]{concept_match_v3_hypothesis_test.png}}
\caption{Concept Match V3 vs. V2 Hypothesis Test on Definition Questions}
\label{fig:concept_match_v3_hypothesis_test}
\end{figure}



\subsection{Concept Match NOT}

Concept Match NOT extends the logic of Concept Match V3, but turns it on its head to handle questions of the type, ``Which of the following is NOT ...'', where for the phrase that follows, all options are true except for one, which is the correct answer.  An example of a question of this type is ``Which of the following is NOT a plea a defendant may enter at an arraignment?" \cite{acfe_study_package_2011}
Concept Match NOT takes an approach that inverts the over-arching approach of the algorithms we've discussed above.  Instead of looking for the option for which there's the greatest affinity between option query result sets and the question stem result set, Concept Match NOT attempts to find the option whose result set has the least affinity.  

Fig.~\ref{fig:concept_match_not_example_part_1} and Fig.~\ref{fig:concept_match_not_example_part_2} show an example of this algorithm at work.  First, as we saw in the agent's justification in earlier algorithms, we see the result set for the question stem query, and then those for the option queries.  Then, the agent, attempts to map the overlap between the question stem result set and the option result sets by building two hash maps.  The first one, the ``docOptionScores" map, associates each document among the option query result sets with the option for which that document earned the highest score.  The algorithm then utilizes this data to construct the second hash map, ``optionScoreDocs", which consists of option/document key/value pairs whose documents are present in both the ``docOptionScores" map and in the question stem result set.  Using this data structure, the agent makes a selection; specifically, it selects the option whose document has the weakest affinity with the question stem result set.  In this case, that's option d, skimming, since this option has no representation in the optionScoreDocs data structure, implying it has no documents that overlap with the result set of the question stem.  (Looking closely, we see that whereas all of the other option queries have result sets that are non-empty, the the result set for the skimming option has no documents, and so, it's reasonable that it has the weakest affinity with the question stem.) 



\begin{figure}
\centering
\vspace{0.75in}
\framebox{\includegraphics[width=125mm, height=125mm]{concept_match_not_example_part_1.png}}
\caption{Concept Match Not Example - Part 1}
\label{fig:concept_match_not_example_part_1}
\end{figure}

\begin{figure}
\centering
\vspace{0.75in}
\framebox{\includegraphics[width=125mm, height=90mm]{concept_match_not_example_part_2.png}}
\caption{Concept Match Not Example - Part 2}
\label{fig:concept_match_not_example_part_2}
\end{figure}

\subsubsection{Concept Match NOT Performance}

Fig.~\ref{fig:concept_match_not_training_set_performance} shows the performance of the Concept Match NOT algorithm on Definition/NOT questions in the training set - 8 out of 27 correct, or 29.6\%.  It is not unreasonable to expect that this algorithm would show weaker performance than its inverted cousin, Concept Match V3, as discovering the negative, as we are attempting to do in the case for this question type, is difficult to do.  In fact, we cannot reject the null hypothesis that this algorithm performs any better on Definition/NOT questions than random guessing, as shown in Fig.~\ref{fig:concept_match_not_hypothesis_test}.  Further investigation is required to refine this algorithm or to take a different approach with these types of questions altogether.


%However, even with that disadvantage, the Concept Match Not algorithm shows statistically significant improvement over random approach at the 99\% confidence level.  Note, we compare this algorithm to random selection because in Version 1 of the CFE Agent we observe that no algorithm actually did any better than random selection for this type of question, refer to Fig.~\ref{fig:version_1_training_set_results_not}.

\begin{figure}
\centering
\vspace{0.75in}
\framebox{\includegraphics[width=110mm, height=16mm]{concept_match_not_training_set_performance.png}}
\caption{Performance of Concept Match Not on Training Set - Definition/NOT Questions}
\label{fig:concept_match_not_training_set_performance}
\end{figure}

\begin{figure}
\centering
\vspace{0.75in}
\framebox{\includegraphics[width=75mm, height=60mm]{concept_match_not_hypothesis_test.png}}
\caption{Concept Match Not vs. Random Hypothesis Test on Definition/Not Questions}
\label{fig:concept_match_not_hypothesis_test}
\end{figure}




\subsection{Concept Match NOTA}

Concept Match NOTA leverages the logic in Concept Match V3 for concept matching, and extends it for addressing definition questions in which the last option is ``none of the above''.  Hereafter, we'll refer to such questions as NOTA questions.

The first concern to investigate in developing this algorithm was the frequency with which the ``none of the above" option was actually the correct response in NOTA questions.  In order to determine this, we developed a trivial algorithm, called the None Of the Above Algorithm, which simply selects the last option, that is, the ``none of the above" option, always.  Then, we ran this algorithm on all 162 NOTA questions in the training set.  The ratio of the correctly answered questions for this algorithm gave us our answer.  Fig.~\ref{fig:nota_training_set_performance} shows the performance results.  We see that the ``none of the above" option is under-represented as a correct answer, serving as the correct answer only 7.4\% of the time.  Whereas we'd expect that it should be the correct answer 25\% of the time, it is, in fact, drastically under-represented as the correct answer.  This served as the inspiration for the simplistic but strategic approach of this algorithm - simply remove the ``none of the above option" as one of the possible options and select from the remaining three options (using the logic of Concept Match V3).

\begin{figure}
\centering
\vspace{0.75in}
\framebox{\includegraphics[width=110mm, height=16mm]{nota_training_set_performance.png}}
\caption{Performance of NOTA Algorithm on NOTA Questions}
\label{fig:nota_training_set_performance}
\end{figure}


Fig.~\ref{fig:concept_match_nota_example} shows an example of the execution of this algorithm.  The agent collects the result set for the question stem query, notices that this question is a NOTA question and thus, removes the ``none of the above" option from its set of options.  Then, it infers from the score of the ``Quiet Rooms" document (id=171) relative to that of the second place document that it's a premier document, and picks the option whose query scores the ``Quiet Rooms" document higher than any other option.  This leads to the correct selection of a) Quiet Rooms.


\begin{figure}
\centering
\vspace{0.75in}
\framebox{\includegraphics[width=125mm, height=110mm]{concept_match_nota_example.png}}
\caption{Concept Match NOTA Example}
\label{fig:concept_match_nota_example}
\end{figure}

\subsubsection{Concept Match NOTA Performance}

Fig.~\ref{fig:concept_match_nota_training_set_performance} shows the performance of this algorithm on the Definition/NOTA questions at 65.8\% accuracy.  Fig.~\ref{fig:concept_match_nota_hypothesis_test} shows a hypothesis test of this algorithm against the Max Frequency algorithm which had the best results in the CFE Agent Version 1.  Since Max Frequency showed results of 56.5\% on 161 questions, the results of Concept Match NOTA were statistically significant at the 99\% confidence level.  Given this, we incorporate this algorithm into the agent as part of its enhanced suite of tools of Version 2.

\begin{figure}
\centering
\vspace{0.75in}
\framebox{\includegraphics[width=110mm, height=16mm]{concept_match_nota_training_set_performance.png}}
\caption{Performance of Concept Match NOTA on Training Set - Definition/NOTA Questions}
\label{fig:concept_match_nota_training_set_performance}
\end{figure}



\begin{figure}
\centering
\vspace{0.75in}
\framebox{\includegraphics[width=100mm, height=75mm]{concept_match_nota_hypothesis_test.png}}
\caption{Concept Match NOTA vs. Max Frequency Hypothesis Test on Definition/NOTA  Questions}
\label{fig:concept_match_nota_hypothesis_test}
\end{figure}





\section{CFE Agent Version 2 Results}

Fig.~\ref{fig:version_2_training_set_performance} summarizes the performance of the CFE Agent Version 2.  It shows that with a 70.0\% accuracy rate on definition questions, Concept Match V3 is the preferred algorithm for that question type.  Interestingly, Concept Match NOT emerges as the preferred algorithm for long-option/NOT questions, with an accuracy rate of 46.7\%, even though it shows no better performance than random guessing on other question types of a similar nature -- definition/NOT  and definition/except.  There are 30 questions in the training set of the long-option/NOT type -- not an insignificant number.  But further investigation is required to ascertain whether/why this particular algorithm is uniquely effective for this question type.  Finally, Concept Match NOTA is the preferred algorithm for NOTA questions, with an accuracy of 65.8\%.

%\begin{figure}
%\centering
%\vspace{0.75in}
%\includegraphics[width=125mm, height=100mm, angle=90]{version_2_training_set_performance.png}
%\caption{Performance of CFE Agent Version 2 on Training Set}
%\label{fig:version_2_training_set_performance}
%\end{figure}

\begin{sidewaysfigure}
\centering
\vspace{0.25in}
\framebox{\includegraphics[width=\textwidth]{version_2_training_set_performance.png}}
\caption{Performance of CFE Agent Version 2 on Training Set}
\label{fig:version_2_training_set_performance}
\end{sidewaysfigure}

Fig.~\ref{fig:version_2_multiple_choice_hypothesis_test} shows the results of a hypothesis test of the performance of the CFE Agent Version 2 on the entire battery of training set questions relative to that of Version 1.  The analysis shows the improvement for Version 2 over Version 1 to be statistically significant at the 99\% level.  This should not be surprising since the question types for which we've targeted our new algorithms, namely Definition, Definition/NOT, and Definition/NOTA , constitute a large portion of the question battery (338 of the 867 questions), as indicated by the Trials column of the table in Fig.~\ref{fig:version_2_training_set_performance}.

\begin{figure}
\centering
\vspace{0.75in}
\framebox{\includegraphics[width=75mm, height=60mm]{version_2_multiple_choice_hypothesis_test.png}}
\caption{CFE Agent Version 2 vs. CFE Agent Version 1 Hypothesis Test on Multiple Choice Questions}
\label{fig:version_2_multiple_choice_hypothesis_test}
\end{figure}

Running the CFE Agent on the test set of 200 questions yields a score of 119 out of 200, or 59.5\%, a dramatic improvement over the 49\% of Version 1.  This shows that the more sophisticated algorithms in Version 2 offer significantly better performance, on the order of 10\% marginal benefit in terms of accuracy, and offer some degree of argument-based justification, though not proof-based.  

















